{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FantaSanremo 2023\n",
    "\n",
    "Social Network, Content, and Trend analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il Festival di Sanremo è sempre più social. E così, anche aziende e content creator seguono gli avvenimenti del festival per restare aggiornato su possibili trend da sfruttare e possibilità per crescere. Negli ultimi anni, il \"FantaSanremo\" ha spopolato tra giovani e meno giovani.\n",
    "\n",
    "Si tratta di un gioco simile al classico e ormai popolare fantacalcio, ma incentrato proprio sul festival. Ogni partecipante sceglie una squadra composta da 5 cantanti con un massimo di 100 'bauli' (prezzo di acquisto), e - in base ai punti raccolti dalla propria squadra - compete con amici e altri in un campionato a punteggio. Per ottenere punti e riconoscimenti, i cantanti devono svolgere alcune attività presenti nel regolamento di ogni anno, senza dimenticare il festival vero e proprio, e quindi il piazzamento in classifica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'interesse nello svolgere questo progetto è quindi raccolto nelle seguenti domande:\n",
    "\n",
    "- Quali sono i trend relativi a Sanremo 2023 e al FantaSanremo?\n",
    "- Come possono essere sfruttati dall'azienda X e/o dall'artista Y per promuoversi (a livello di brand)\n",
    "- Qual è il sentiment generale in merito al tema?\n",
    "- Ѐ possibile individuare community?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il festival è \"sfruttato\" dai brand per farsi conoscere e/o fidelizzare i (possibili) clienti, e dagli artisti per aumentare la propria fanbase o la propria popolarità, oltre ovviamente a competere con la propria canzone per raggiungere il successo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli artisti, in particolare, guadagnano sempre più grazie agli streaming delle varie piattaforme musicali, guadagni che aumentano con il numero di ascolti delle proprie canzoni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23 = pd.read_csv('../data/raw/fantasanremo2023_scraping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fantasanremo23.shape)\n",
    "print()\n",
    "print(fantasanremo23.columns)\n",
    "print()\n",
    "\n",
    "fantasanremo23.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset, ottenuto tramite web scraping grazie a \"snscraper\", è composto da poco più di 28000 righe e 6 colonne.\n",
    "\n",
    "Il periodo di riferimento, invece, va dal 25/12/2022 al 12/02/2023. Il 25 dicembre corrisponde al giorno precedente all'annuncio dei 'bauli' (valore) necessari per acquistare ciascun cantante nella propria squadra; mentre il 12 febbraio corrisponde al giorno successivo dell'annuncio della classifica finale del FantaSanremo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.text import OffsetFrom\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23 = fantasanremo23[['date', 'id', 'content', 'username', 'like_count', 'retweet_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "fantasanremo23.drop_duplicates(subset =\"id\", inplace = True)\n",
    "fantasanremo23.reset_index(drop = True, inplace = True)\n",
    "fantasanremo23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23['date'] = pd.to_datetime(fantasanremo23['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min date', fantasanremo23.date.min())\n",
    "print('max date', fantasanremo23.date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change date format\n",
    "day = fantasanremo23['date'].dt.day\n",
    "month = fantasanremo23['date'].dt.month\n",
    "year = fantasanremo23['date'].dt.year\n",
    "\n",
    "date = year.astype(str) + month.astype(str).str.zfill(2) + day.astype(str).str.zfill(2)\n",
    "date = pd.to_datetime(date, format='%Y%m%d')\n",
    "fantasanremo23.drop(columns = ['date'], inplace = True)\n",
    "fantasanremo23['date'] = date\n",
    "\n",
    "# reorder columns\n",
    "cols = fantasanremo23.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "fantasanremo23 = fantasanremo23[cols].copy()\n",
    "\n",
    "print('Tweet per day:')\n",
    "print()\n",
    "print(fantasanremo23.groupby('date').count()['id'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(10,5), facecolor='#EFE9E6')\n",
    "ax.set_facecolor('#EFE9E6')\n",
    "\n",
    "plt.plot(fantasanremo23.groupby('date').id.nunique(), color='#5A5A5A')\n",
    "\n",
    "# annotation with data coordinates and offset points\n",
    "ax.annotate(\n",
    "    xy = (140, 250),\n",
    "    xycoords = 'figure pixels',\n",
    "    xytext = (10, 15),\n",
    "    textcoords = 'offset pixels',\n",
    "    text = '27 December 2022\\nRegistration Openings',\n",
    "    size = 6,\n",
    "    color = \"grey\",\n",
    "    arrowprops = dict(\n",
    "        arrowstyle=\"->\", shrinkA=0, shrinkB=5, color=\"grey\", linewidth=0.75,\n",
    "        connectionstyle=\"angle3,angleA=50,angleB=-30\"\n",
    "    ) # arrow to connect annotation\n",
    ")\n",
    "\n",
    "# annotation with data coordinates and offset points\n",
    "ax.annotate(\n",
    "    xy = (740, 440),\n",
    "    xycoords = 'figure pixels',\n",
    "    xytext = (15, 15),\n",
    "    textcoords = 'offset pixels',\n",
    "    text = '7 February 2023\\nStart of the Show',\n",
    "    size = 6,\n",
    "    color = \"grey\",\n",
    "    arrowprops = dict(\n",
    "        arrowstyle=\"->\", shrinkA=0, shrinkB=5, color=\"grey\", linewidth=0.75,\n",
    "        connectionstyle=\"angle3,angleA=50,angleB=-30\"\n",
    "    ) # arrow to connect annotation\n",
    ")\n",
    "\n",
    "# spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# format x axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m\\n%Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5))\n",
    "\n",
    "ax.yaxis.grid(linestyle='dashed', alpha=0.5)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Tweets')\n",
    "ax.set_title('FantaSanremo2023 - Number of Tweets per Date')\n",
    "\n",
    "plt.savefig('../figures/numTweetsDate.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come possiamo osservare dalla distribuzione di tweet nei vari giorni, i giorni in cui sono stati pubblicati la maggior parte di tweet corrispondono ai giorni del festival o a giorni di annunci particolari, il 2022-12-27 è, ad esempio, il giorno di apertura delle iscrizioni per il FantaSanremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with the authors of the tweets and their respective frequency\n",
    "freq_authors = fantasanremo23['username'].value_counts()\n",
    "freq_authors.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approfondiamo ora, più nel dettaglio, i tweet risalenti al 27-12-2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data\n",
    "fantasanremo23_271222 = fantasanremo23[fantasanremo23.date == '2022-12-27'].copy()\n",
    "fantasanremo23_271222.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique tweets\n",
    "fantasanremo23_271222.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.scatterplot(\n",
    "    data=fantasanremo23_271222,\n",
    "    x='retweet_count',\n",
    "    y='like_count',\n",
    "    s=200\n",
    ")\n",
    "\n",
    "for i, point in fantasanremo23_271222[\n",
    "    (fantasanremo23_271222.like_count > 1000) | (fantasanremo23_271222.retweet_count > 1000)].iterrows():\n",
    "    ax.text(point.retweet_count*1.03, point.like_count*1, point.username + ': ' + point.content)\n",
    "\n",
    "ax.set_xlabel('Number of Retweets')\n",
    "ax.set_ylabel('Number of Likes')\n",
    "plt.savefig('../figures/scatterLikeRT.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23['new_content'] = fantasanremo23['content'].astype(str)\n",
    "\n",
    "# lower case\n",
    "fantasanremo23['new_content'] = fantasanremo23['new_content'].str.lower()\n",
    "\n",
    "# remove punctuation but keep hashtags\n",
    "fantasanremo23['new_content'] = fantasanremo23['new_content'].str.replace(r'^\\w#\\s','', regex=True)\n",
    "\n",
    "# remove stopwords\n",
    "stop = stopwords.words('italian')\n",
    "fantasanremo23['new_content'] = fantasanremo23['new_content'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "fantasanremo23[\"new_content\"] = fantasanremo23[\"new_content\"].apply(lambda text: lemmatize_words(text))\n",
    "\n",
    "# remove links\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "fantasanremo23[\"new_content\"] = fantasanremo23[\"new_content\"].apply(lambda text: remove_urls(text))\n",
    "\n",
    "# remove html\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "fantasanremo23[\"new_content\"] = fantasanremo23[\"new_content\"].apply(lambda text: remove_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23['new_content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtags\n",
    "text = ' '.join(tweet for tweet in fantasanremo23.new_content)\n",
    "\n",
    "# extract hashtags\n",
    "hashtags = re.findall('#(\\w+)', text)\n",
    "\n",
    "ax = pd.DataFrame(hashtags, columns=['hashtag']).value_counts().head(20).plot(kind='barh', figsize=(3,5))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.savefig('../figures/hashtags.png', dpi=300, bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a wordcloud for the combined text\n",
    "hashtags = ' '.join(hashtag for hashtag in hashtags)\n",
    "\n",
    "# setup, generate and save the word cloud image to a file\n",
    "wc = WordCloud(\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    random_state=0,\n",
    "    max_font_size=110,\n",
    "    background_color='white',\n",
    "    collocations=False).generate(hashtags)\n",
    "\n",
    "wc.to_file(\"../figures/WordCloud_hashtags.png\")\n",
    "\n",
    "# show the wordcloud as output\n",
    "plt.figure()\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stesso procedimento, stavolta escludendo hashtag superflui o non interessanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtags\n",
    "text2 = ' '.join(tweet for tweet in fantasanremo23.new_content)\n",
    "\n",
    "# extract hashtags\n",
    "hashtags2 = re.findall('#(\\w+)', text2)\n",
    "\n",
    "hashtags2 = [\n",
    "    hashtag.strip() for hashtag in hashtags2 if hashtag not in\n",
    "    [\n",
    "        'fantasanremo2023',\n",
    "        'fantasanremo',\n",
    "        'sanremo2023',\n",
    "        'sanremo',\n",
    "        'sanremo23',\n",
    "        'festivaldisanremo',\n",
    "        'festivaldisanremo2023',\n",
    "        'fantasanremo23'\n",
    "    ]\n",
    "]\n",
    "\n",
    "ax = pd.DataFrame(hashtags2, columns=['hashtag']).value_counts().head(20).plot(kind='barh', figsize=(3,5))\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.savefig('../figures/hashtags2.png', dpi=300, bbox_inches = \"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a wordcloud for the combined text\n",
    "hashtags2 = ' '.join(hashtag for hashtag in hashtags2)\n",
    "\n",
    "# setup, generate and save the word cloud image to a file\n",
    "wc = WordCloud(\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    random_state=0,\n",
    "    max_font_size=110,\n",
    "    background_color='white',\n",
    "    collocations=False).generate(hashtags2)\n",
    "\n",
    "wc.to_file(\"../figures/WordCloud_hashtags2.png\")\n",
    "\n",
    "# show the wordcloud as output\n",
    "plt.figure()\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23.to_csv('../data/fantasanremo23.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23 = pd.read_csv('../data/fantasanremo23.csv', index_col=0)\n",
    "fantasanremo23.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sa = fantasanremo23.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Sentiment Analysis_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter-XLM-roBERTa-base for Sentiment Analysis\n",
    "\n",
    "This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages.\n",
    "\n",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text (username and link placeholder)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "model.save_pretrained(MODEL)\n",
    "tokenizer.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sa['scores'] = fantasanremo23_sa.content.apply(lambda x: softmax(model(**tokenizer(preprocess(x), return_tensors='pt'))[0][0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sa[['content', 'scores']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sa['negative'] = fantasanremo23_sa.scores.apply(lambda x: x[0])\n",
    "fantasanremo23_sa['neutral'] = fantasanremo23_sa.scores.apply(lambda x: x[1])\n",
    "fantasanremo23_sa['positive'] = fantasanremo23_sa.scores.apply(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sa.to_csv('../data/fantasanremo23_sa.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sn = fantasanremo23.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract user mentions\n",
    "fantasanremo23_sn['user_mentions'] = fantasanremo23_sn['content'].str.extract('@(\\S+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sn['user_mentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "fantasanremo23_sn['user_mentions'] = fantasanremo23_sn['user_mentions'].str.lower()\n",
    "\n",
    "# define a string of punctuation characters to remove\n",
    "punctuation = string.punctuation\n",
    "\n",
    "# Use str.rstrip to remove punctuation from the end of the \"text\" column\n",
    "fantasanremo23_sn['user_mentions'] = fantasanremo23_sn['user_mentions'].str.rstrip(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasanremo23_sn.to_csv('../data/fantasanremo23_sn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_mentions = fantasanremo23_sn[['username', 'user_mentions']]\n",
    "author_mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_mentions.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Takeaways"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gli hashtags più utilizzati tra i tweet relativi al #FantaSanremo 2023 riguardano i cantanti in gara"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "378175c2c6ec7d4fc364b1674a9631fcafdf0526712f6e898c5a9c4fd1a3d1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
